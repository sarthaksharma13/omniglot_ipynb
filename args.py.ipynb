{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Custom FloatRange class, to check for float argument ranges\n",
    "class FloatRange(object):\n",
    "\tdef __init__(self, start, end):\n",
    "\t\tself.start = start\n",
    "\t\tself.end = end\n",
    "\tdef __eq__(self, other):\n",
    "\t\treturn self.start <= other <= self.end\n",
    "\n",
    "\n",
    "################ Model Options ################################\n",
    "parser.add_argument('-initType', help = 'Weight initialization for the linear layers', \\\n",
    "\ttype = str.lower, choices = ['xavier'], default = 'xavier')\n",
    "parser.add_argument('-activation', help = 'Activation function to be used', type = str.lower, \\\n",
    "\tchoices = ['relu', 'selu'], default = 'selu')\n",
    "parser.add_argument('-dropout', help = 'Drop ratio of dropout at penultimate linear layer, \\\n",
    "\tif dropout is to be used.', type = float, choices = [FloatRange(0.0, 1.0)])\n",
    "parser.add_argument('-imageWidth', help = 'Width of the input image', type = int, default = 64)\n",
    "parser.add_argument('-imageHeight', help = 'Height of the input image', type = int, default = 64)\n",
    "\n",
    "################ Dataset ######################################\n",
    "parser.add_argument('-dataset', help = 'dataset to be used for training the network', default = 'omniglot')\n",
    "\n",
    "################### Hyperparameters ###########################\n",
    "parser.add_argument('-lr', help = 'Learning rate', type = float, default = 0.00006)\n",
    "parser.add_argument('-momentum', help = 'Momentum', type = float, default = 0.009)\n",
    "parser.add_argument('-weightDecay', help = 'Weight decay', type = float, default = 0.)\n",
    "parser.add_argument('-lrDecay', help = 'Learning rate decay factor', type = float, default = 0.)\n",
    "parser.add_argument('-iterations', help = 'Number of iterations after loss is to be computed', \\\n",
    "\ttype = int, default = 100)\n",
    "parser.add_argument('-beta1', help = 'beta1 for ADAM optimizer', type = float, default = 0.9)\n",
    "parser.add_argument('-beta2', help = 'beta2 for ADAM optimizer', type = float, default = 0.999)\n",
    "parser.add_argument('-gradClip', help = 'Max allowed magnitude for the gradient norm, \\\n",
    "\tif gradient clipping is to be performed. (Recommended: 1.0)', type = float)\n",
    "\n",
    "# parser.add_argument('-crit', help = 'Error criterion', default = 'MSE')\n",
    "parser.add_argument('-optMethod', help = 'Optimization method : adam | sgd | adagrad ', \\\n",
    "\ttype = str.lower, choices = ['adam', 'sgd', 'adagrad'], default = 'adam')\n",
    "parser.add_argument('-nepochs', help = 'Number of epochs', type = int, default = 200)\n",
    "parser.add_argument('-trainBatch', help = 'train batch size', type = int, default = 128)\n",
    "parser.add_argument('-validBatch', help = 'valid batch size', type = int, default = 1)\n",
    "parser.add_argument('-gamma', help = 'For L2 regularization', \\\n",
    "\ttype = float, default = 0.0)\n",
    "parser.add_argument('-iters',type = int, default =90000)\n",
    "\n",
    "\n",
    "################### Paths #####################################\n",
    "parser.add_argument('-cachedir', \\\n",
    "\thelp = '(Relative path to) directory in which to store logs, models, plots, etc.', \\\n",
    "\ttype = str, default = 'cache')\n",
    "###### Experiments, Snapshots, and Visualization #############\n",
    "parser.add_argument('-expID', help = 'experiment ID', default = 'tmp')\n",
    "parser.add_argument('-snapshot', help = 'when to take model snapshots', type = int, default = 5)\n",
    "parser.add_argument('-snapshotStrategy', help = 'Strategy to save snapshots. Note that this has \\\n",
    "\tprecedence over the -snapshot argument. 1. none: no snapshot at all | 2. default: as frequently \\\n",
    "\tas specified in -snapshot | 3. best: keep only the best performing model thus far', \\\n",
    "\ttype = str.lower, choices = ['none', 'default', 'best'])\n",
    "parser.add_argument('-tensorboardX', help = 'Whether or not to use tensorboardX for \\\n",
    "\tvisualization', type = bool, default = True)\n",
    "\n",
    "########### Debugging, Profiling, etc. #######################\n",
    "parser.add_argument('-debug', help = 'Run in debug mode, and execute 3 quick iterations per train \\\n",
    "\tloop. Used in quickly testing whether the code has a silly bug.', type = bool, default = False)\n",
    "parser.add_argument('-profileGPUUsage', help = 'Profiles GPU memory usage and prints it every \\\n",
    "\ttrain/val batch', type = bool, default = False)\n",
    "parser.add_argument('-sbatch', help = 'Replaces tqdm and print operations with file writes when \\\n",
    "\tTrue. Useful for reducing I/O when not running in interactive mode (eg. on clusters)', type = bool)\n",
    "\n",
    "################### Reproducibility ##########################\n",
    "parser.add_argument('-randomseed', help = 'Seed for pseudorandom number generator', \\\n",
    "\ttype = int, default = 12345)\n",
    "parser.add_argument('-isDeterministic', help = 'Whether or not the code should \\\n",
    "\tuse the provided random seed and run deterministically', type = bool, default = False)\n",
    "parser.add_argument('-numworkers', help = 'Number of threads available to the DataLoader', \\\n",
    "\ttype = int, default = 1)\n",
    "\n",
    "\n",
    "arguments = parser.parse_args()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
